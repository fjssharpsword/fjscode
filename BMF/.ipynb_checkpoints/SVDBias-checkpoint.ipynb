{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Iteration: 10 ; error = 1771.6037\n",
      "Iteration: 20 ; error = 1666.4704\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sqrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2bb40f1e5736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0msquaredError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquaredError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquaredError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE@{}:{} at Dataset {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqrt' is not defined"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.29\n",
    "@function: baseline SVDBias \n",
    "           Datatset: \n",
    "           MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, num_ng=2):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.num_ng = num_ng\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        pos_samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        #smapling the negative items\n",
    "        neg_samples = random.sample([\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] == 0\n",
    "        ], len(pos_samples)*num_ng)\n",
    "        \n",
    "        self.samples = pos_samples + neg_samples\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    svdb = SVDBias(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi), num_ng=2)#negative sample ratio\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        nR = svdb.train(K=K, alpha=0.001, beta=0.01, epochs=20)\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - nR[int(u)][int(i)]\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             user        item  rating        time\n",
      "0  A28O3NP6WR5517  0188399313     5.0  1369612800\n",
      "1   AX0M1Z6ZWO52J  0188399399     5.0  1365465600\n",
      "2  A1KD7N84L7NIUT  0188399518     4.0  1392336000\n",
      "3  A29CUDEIF4X1UO  0188399518     3.0  1373241600\n",
      "4  A32592TYN6C9EM  0316967297     4.0  1378425600\n",
      "531889\n",
      "64425\n"
     ]
    }
   ],
   "source": [
    "#dataset:Amazon Product-dm(digital music):http://jmcauley.ucsd.edu/data/amazon/index.html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "filePath = '/data/tmpexec/ratings_Baby.csv'\n",
    "data = pd.read_csv(filePath, sep=',', header=None, names=['user', 'item', 'rating','time'], \\\n",
    "                                 usecols=[0, 1, 2,3], dtype={0: np.str, 1: np.str, 2: np.float,3:str})\n",
    "print (data.head())\n",
    "ui=data[['user','item']]\n",
    "le = LabelEncoder()\n",
    "ui = ui.apply(le.fit_transform)#label the user and item no\n",
    "print (ui['user'].max())\n",
    "print (ui['item'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
