{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5.7779e+07: 100%|██████████| 1000/1000 [23:40<00:00,  1.39s/it]\n",
      "Finished [100%]: Average Loss = 5.7701e+07\n",
      "I0731 04:00:43.768507 140096310822656 inference.py:248] Finished [100%]: Average Loss = 5.7701e+07\n",
      "100%|█████████▉| 498/500 [44:09<00:14,  7.15s/it]"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.31\n",
    "@function: baseline BPMF(Bayesian Probabilistic Matrix Factorization)\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import RandomState\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "'''    \n",
    "def build_bpmf_model(train, dim, alpha=2, std=0.01):\n",
    "    \"\"\"Build the modified BPMF model using pymc3. The original model uses\n",
    "    Wishart priors on the covariance matrices. Unfortunately, the Wishart\n",
    "    distribution in pymc3 is currently not suitable for sampling. This\n",
    "    version decomposes the covariance matrix into:\n",
    "        diag(sigma) \\dot corr_matrix \\dot diag(std).\n",
    "    We use uniform priors on the standard deviations (sigma) and LKJCorr\n",
    "    priors on the correlation matrices (corr_matrix):\n",
    "        sigma ~ Uniform\n",
    "        corr_matrix ~ LKJCorr(n=1, p=dim)\n",
    "    \"\"\"\n",
    "    n, m = train.shape\n",
    "    beta_0 = 1  # scaling factor for lambdas; unclear on its use\n",
    " \n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    " \n",
    "    # We will use separate priors for sigma and correlation matrix.\n",
    "    # In order to convert the upper triangular correlation values to a\n",
    "    # complete correlation matrix, we need to construct an index matrix:\n",
    "    n_elem = int(dim * (dim - 1) / 2)\n",
    "    tri_index = np.zeros([dim, dim], dtype=int)\n",
    "    tri_index[np.triu_indices(dim, k=1)] = np.arange(n_elem)\n",
    "    tri_index[np.triu_indices(dim, k=1)[::-1]] = np.arange(n_elem)\n",
    " \n",
    "    logging.info('building the BPMF model')\n",
    "    with pm.Model() as bpmf:\n",
    "        # Specify user feature matrix\n",
    "        sigma_u = pm.Uniform('sigma_u', shape=dim)\n",
    "        corr_triangle_u = pm.LKJCorr('corr_u', n=1, p=dim, testval=np.random.randn(n_elem) * std)\n",
    " \n",
    "        corr_matrix_u = corr_triangle_u[tri_index]\n",
    "        corr_matrix_u = tt.fill_diagonal(corr_matrix_u, 1)\n",
    "        cov_matrix_u = tt.diag(sigma_u).dot(corr_matrix_u.dot(tt.diag(sigma_u)))\n",
    "        lambda_u = tt.nlinalg.matrix_inverse(cov_matrix_u)\n",
    " \n",
    "        mu_u = pm.Normal('mu_u', mu=0, tau=beta_0 * tt.diag(lambda_u), shape=dim,testval=np.random.randn(dim) * std)\n",
    "        U = pm.MvNormal('U', mu=mu_u, tau=lambda_u, shape=(n, dim),testval=np.random.randn(n, dim) * std)\n",
    " \n",
    "        # Specify item feature matrix\n",
    "        sigma_v = pm.Uniform('sigma_v', shape=dim)\n",
    "        corr_triangle_v = pm.LKJCorr('corr_v', n=1, p=dim,testval=np.random.randn(n_elem) * std)\n",
    " \n",
    "        corr_matrix_v = corr_triangle_v[tri_index]\n",
    "        corr_matrix_v = tt.fill_diagonal(corr_matrix_v, 1)\n",
    "        cov_matrix_v = tt.diag(sigma_v).dot(corr_matrix_v.dot(tt.diag(sigma_v)))\n",
    "        lambda_v = tt.nlinalg.matrix_inverse(cov_matrix_v)\n",
    " \n",
    "        mu_v = pm.Normal('mu_v', mu=0, tau=beta_0 * tt.diag(lambda_v), shape=dim,testval=np.random.randn(dim) * std)\n",
    "        V = pm.MvNormal( 'V', mu=mu_v, tau=lambda_v, shape=(m, dim),testval=np.random.randn(m, dim) * std)\n",
    " \n",
    "        # Specify rating likelihood function\n",
    "        R = pm.Normal('R', mu=tt.dot(U, V.T), tau=alpha * np.ones((n, m)),observed=train)\n",
    " \n",
    "    logging.info('done building the BPMF model')\n",
    "    return bpmf\n",
    "'''\n",
    "def build_bpmf_model(train, dim, alpha=2, std=0.01):\n",
    "    # Mean value imputation on training data.\n",
    "    train = train.copy()\n",
    "    nan_mask = np.isnan(train)\n",
    "    train[nan_mask] = train[~nan_mask].mean()\n",
    " \n",
    "    # Low precision reflects uncertainty; prevents overfitting.\n",
    "    # We use point estimates from the data to intialize.\n",
    "    # Set to mean variance across users and items.\n",
    "    alpha_u = 1 / train.var(axis=1).mean()\n",
    "    alpha_v = 1 / train.var(axis=0).mean()\n",
    " \n",
    "    logging.info('building the BPMF model')\n",
    "    n, m = train.shape\n",
    "    with pm.Model() as bpmf:\n",
    "        U = pm.MvNormal('U', mu=0, tau=alpha_u * np.eye(dim),shape=(n, dim), testval=np.random.randn(n, dim) * std)\n",
    "        V = pm.MvNormal('V', mu=0, tau=alpha_v * np.eye(dim),shape=(m, dim), testval=np.random.randn(m, dim) * std)\n",
    "        R = pm.Normal('R', mu=tt.dot(U, V.T), tau=alpha * np.ones(train.shape),observed=train)\n",
    "    logging.info('done building BPMF model')\n",
    "    return bpmf\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\\\n",
    "    R = ds.list_to_matrix(ds.trainset, ds.maxu, ds.maxi)#get matrix\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        bpmf = build_bpmf_model(train=R, dim=K)#dim is the number of latent factors\n",
    "        with bpmf:# sample with BPMF\n",
    "            tstart = time.time()\n",
    "            logging.info('Starting BPMF training')\n",
    "            approx = pm.fit(n=1000, method=pm.ADVI())\n",
    "            trace = approx.sample(draws=500)\n",
    "            #start = pm.find_MAP()    \n",
    "            #step = pm.NUTS()\n",
    "            #trace = pm.sample(1000, step=step, start=start)\n",
    "            elapsed = time.time() - tstart    \n",
    "            logging.info('Completed BPMF in %d seconds' % int(elapsed))\n",
    "        \n",
    "        with bpmf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "            nR = np.mean(ppc['R'],0)#three dims, calcuate the mean with the first dim for posterior\n",
    "\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - nR[int(u)][int(i)]\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
