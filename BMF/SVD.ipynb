{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Iteration: 10 ; error = 1005.3532\n",
      "Iteration: 20 ; error = 912.5285\n",
      "RMSE@20:1.0284452253214498\n",
      "Iteration: 10 ; error = 998.2090\n",
      "Iteration: 20 ; error = 913.7349\n",
      "Iteration: 30 ; error = 890.0635\n",
      "Iteration: 40 ; error = 869.1397\n",
      "Iteration: 50 ; error = 851.8188\n",
      "RMSE@50:0.955703536096593\n",
      "Iteration: 10 ; error = 1003.8942\n",
      "Iteration: 20 ; error = 912.2850\n",
      "Iteration: 30 ; error = 886.4279\n",
      "Iteration: 40 ; error = 867.3193\n",
      "Iteration: 50 ; error = 851.8308\n",
      "Iteration: 60 ; error = 838.8305\n",
      "Iteration: 70 ; error = 828.4326\n",
      "Iteration: 80 ; error = 820.2506\n",
      "RMSE@80:0.9316439192007158\n",
      "Iteration: 10 ; error = 1001.4356\n",
      "Iteration: 20 ; error = 914.1209\n",
      "Iteration: 30 ; error = 890.5863\n",
      "Iteration: 40 ; error = 869.8515\n",
      "Iteration: 50 ; error = 853.3925\n",
      "Iteration: 60 ; error = 840.5416\n",
      "Iteration: 70 ; error = 830.3988\n",
      "Iteration: 80 ; error = 822.2616\n",
      "Iteration: 90 ; error = 815.6949\n",
      "Iteration: 100 ; error = 810.3301\n",
      "RMSE@100:0.922784357240591\n",
      "Iteration: 10 ; error = 998.4142\n",
      "Iteration: 20 ; error = 909.7497\n",
      "Iteration: 30 ; error = 882.6962\n",
      "Iteration: 40 ; error = 863.3262\n",
      "Iteration: 50 ; error = 848.0797\n",
      "Iteration: 60 ; error = 836.0847\n",
      "Iteration: 70 ; error = 826.7331\n",
      "Iteration: 80 ; error = 819.4484\n",
      "Iteration: 90 ; error = 813.6173\n",
      "Iteration: 100 ; error = 808.9833\n",
      "Iteration: 110 ; error = 805.1819\n",
      "Iteration: 120 ; error = 802.0603\n",
      "Iteration: 130 ; error = 799.4346\n",
      "Iteration: 140 ; error = 797.1912\n",
      "Iteration: 150 ; error = 795.2793\n",
      "RMSE@150:0.9080816426181043\n",
      "Iteration: 10 ; error = 1009.5610\n",
      "Iteration: 20 ; error = 912.6368\n",
      "Iteration: 30 ; error = 886.1726\n",
      "Iteration: 40 ; error = 865.1726\n",
      "Iteration: 50 ; error = 848.5419\n",
      "Iteration: 60 ; error = 835.8019\n",
      "Iteration: 70 ; error = 826.2308\n",
      "Iteration: 80 ; error = 818.7067\n",
      "Iteration: 90 ; error = 812.8746\n",
      "Iteration: 100 ; error = 808.1637\n",
      "Iteration: 110 ; error = 804.3541\n",
      "Iteration: 120 ; error = 801.1940\n",
      "Iteration: 130 ; error = 798.5858\n",
      "Iteration: 140 ; error = 796.3460\n",
      "Iteration: 150 ; error = 794.4242\n",
      "Iteration: 160 ; error = 792.8255\n",
      "Iteration: 170 ; error = 791.3709\n",
      "Iteration: 180 ; error = 790.1487\n",
      "Iteration: 190 ; error = 789.0186\n",
      "Iteration: 200 ; error = 788.0299\n",
      "RMSE@200:0.9106500049471933\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.13\n",
    "@function: baseline: SVD \n",
    "           paper: https://dl.acm.org/citation.cfm?id=1401944\n",
    "           Datatset: KnowledgeBase-CC\n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr  \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "class SVD():\n",
    "    \n",
    "    def __init__(self, R):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.P.dot(self.Q.T)    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    svd = SVD(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi))\n",
    "    for epochs in [20, 50, 80, 100, 150, 200]:\n",
    "        nR = svd.train(K=8, alpha=0.001, beta=0.01, epochs=epochs)\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - nR[int(u)][int(i)]\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(epochs, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Iteration: 10 ; error = 998.2688\n",
      "Iteration: 20 ; error = 913.8993\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-27070209dcaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0msquaredError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquaredError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquaredError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE@{}:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mmapu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.12\n",
    "@function: baseline: SVD \n",
    "           paper: https://dl.acm.org/citation.cfm?id=1401944\n",
    "           Datatset: KnowledgeBase-CC\n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr  \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "class SVD():\n",
    "    \n",
    "    def __init__(self, R):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.P.dot(self.Q.T)\n",
    "    \n",
    "    def map_point(self):\n",
    "        u=100\n",
    "        i=100\n",
    "        return self.P[100,:], self.Q[100,:]\n",
    "        \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    svd = SVD(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi))\n",
    "    #for K in [8, 16, 32, 64]:\n",
    "    nR = svd.train(K=8, alpha=0.001, beta=0.01, epochs=20)\n",
    "    squaredError = []\n",
    "    for u,i,r in ds.testset:\n",
    "        error=r - nR[int(u)][int(i)]\n",
    "        squaredError.append(error * error)\n",
    "    rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "    print(\"RMSE@{}:{}\".format(K, rmse))\n",
    "    \n",
    "    mapu, mapi = svd.map_point()\n",
    "    print(mapu)\n",
    "    print(mapi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.59723545  0.93588047 -0.8052345  -0.50565011 -0.28202841  0.18825349\n",
      " -0.32338122  0.12781368]\n",
      "[-1.84968092  0.77268257 -1.06162059 -0.67087349 -0.35271768  0.13902233\n",
      " -0.51233233  0.083131  ]\n"
     ]
    }
   ],
   "source": [
    "mapu, mapi = svd.map_point()\n",
    "print(mapu)\n",
    "print(mapi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 2313189, User = 10216, Item = 96324, Sparsity = 0.0024\n",
      "Iteration: 10 ; error = 2958.0787\n",
      "Iteration: 20 ; error = 2491.4383\n",
      "RMSE@8:1.1912341121135577\n",
      "Iteration: 10 ; error = 2962.1034\n",
      "Iteration: 20 ; error = 2483.1727\n",
      "RMSE@16:1.1801219373872542\n",
      "Iteration: 10 ; error = 3056.9708\n",
      "Iteration: 20 ; error = 2494.2767\n",
      "RMSE@32:1.1814908620281692\n",
      "Iteration: 10 ; error = 3134.0783\n",
      "Iteration: 20 ; error = 2503.6973\n",
      "RMSE@64:1.1831746929735494\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.08.09\n",
    "@function: baseline: SVD \n",
    "           paper: https://dl.acm.org/citation.cfm?id=1401944\n",
    "           Datatset: KnowledgeBase-CC\n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/kbcc_trainset.csv\" \n",
    "        data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "        maxu, maxi, maxr = data['csr'].max()+1, data['ke'].max()+1, data['num'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/kbcc_testset.csv\" \n",
    "        data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "class SVD():\n",
    "    \n",
    "    def __init__(self, R):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.P.dot(self.Q.T)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    svd = SVD(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi))\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        nR = svd.train(K=K, alpha=0.001, beta=0.01, epochs=20)\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - nR[int(u)][int(i)]\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Iteration: 10 ; error = 1006.5114\n",
      "Iteration: 20 ; error = 909.9645\n",
      "RMSE@8:1.0274106224677013\n",
      "Iteration: 10 ; error = 1005.6342\n",
      "Iteration: 20 ; error = 911.0174\n",
      "RMSE@16:1.0256644829855082\n",
      "Iteration: 10 ; error = 1011.9531\n",
      "Iteration: 20 ; error = 915.0738\n",
      "RMSE@32:1.0278929815768052\n",
      "Iteration: 10 ; error = 1019.6487\n",
      "Iteration: 20 ; error = 917.9371\n",
      "RMSE@64:1.0301750686414972\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.31\n",
    "@function: baseline: SVD \n",
    "           paper: https://dl.acm.org/citation.cfm?id=1401944\n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "class SVD():\n",
    "    \n",
    "    def __init__(self, R):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        self.samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.P.dot(self.Q.T)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    svd = SVD(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi))\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        nR = svd.train(K=K, alpha=0.001, beta=0.01, epochs=20)\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - nR[int(u)][int(i)]\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Iteration: 10 ; error = 1824.7837\n",
      "Iteration: 20 ; error = 1636.5439\n",
      "RMSE@8:2.371066019150275\n",
      "Iteration: 10 ; error = 1827.0909\n",
      "Iteration: 20 ; error = 1628.0322\n",
      "RMSE@16:2.3629720285561637\n",
      "Iteration: 10 ; error = 1840.4726\n",
      "Iteration: 20 ; error = 1641.2772\n",
      "RMSE@32:2.3702191930933796\n",
      "Iteration: 10 ; error = 1844.0530\n",
      "Iteration: 20 ; error = 1663.1879\n",
      "RMSE@64:2.387037792814061\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.31\n",
    "@function: baseline SVD \n",
    "           Datatset: MovieLens-1m:https://grouplens.org/datasets/movielens/  \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self):\n",
    "        self.trainset, self.testset, self.maxu, self.maxi, self.maxr = self._getDataset_as_list()\n",
    "        \n",
    "    def _getDataset_as_list(self):\n",
    "        #trainset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.train.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        maxu, maxi, maxr = data['user'].max()+1, data['item'].max()+1, data['rating'].max()\n",
    "        print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "        trainset = data.values.tolist()\n",
    "        #testset\n",
    "        filePath = \"/data/fjsdata/BMF/ml-1m.test.rating\" \n",
    "        data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "        testset = data.values.tolist()\n",
    "        return trainset, testset, maxu, maxi, maxr \n",
    "    \n",
    "    def list_to_matrix(self, dataset, maxu, maxi):              \n",
    "        dataMat = np.zeros([maxu, maxi], dtype=np.float32)\n",
    "        for u,i,r in dataset:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "class SVD():\n",
    "    \n",
    "    def __init__(self, R, num_ng=2):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.num_ng = num_ng\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        pos_samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        #smapling the negative items\n",
    "        neg_samples = random.sample([\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] == 0\n",
    "        ], len(pos_samples)*num_ng)\n",
    "        \n",
    "        self.samples = pos_samples + neg_samples\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.P.dot(self.Q.T)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    ds = DataSet()#loading dataset\n",
    "    svd = SVD(R=ds.list_to_matrix(ds.trainset,ds.maxu,ds.maxi), num_ng=2)#negative sample ratio\n",
    "    for K in [8, 16, 32, 64]:\n",
    "        nR = svd.train(K=K, alpha=0.001, beta=0.01, epochs=20)\n",
    "        squaredError = []\n",
    "        for u,i,r in ds.testset:\n",
    "            error=r - nR[int(u)][int(i)]\n",
    "            squaredError.append(error * error)\n",
    "        rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "        print(\"RMSE@{}:{}\".format(K, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
