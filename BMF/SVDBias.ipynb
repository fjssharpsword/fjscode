{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8  \n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.29\n",
    "@function: baseline SVDBias \n",
    "           Datatset: Movielen-1m, Pinterest-20, KnowledgeBase-CC \n",
    "           Evaluation: RMSE\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import heapq\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName):\n",
    "        self.dataList, self.maxu, self.maxi = self.getDataset_as_list(fileName)\n",
    "        \n",
    "    def getDataset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            #data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.dataList:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDBias():\n",
    "    \n",
    "    def __init__(self, R, num_ng=2):\n",
    "        \"\"\"\n",
    "        Perform matrix factorization to predict empty entries in a matrix.     \n",
    "        Arguments\n",
    "        - R (ndarray)   : user-item rating matrix\n",
    "        - num_ng (int)  : number of negative items\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.num_users, self.num_items = R.shape\n",
    "        self.num_ng = num_ng\n",
    "        \n",
    "        # Create a list of training samples\n",
    "        pos_samples = [\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] > 0\n",
    "        ]\n",
    "        #smapling the negative items\n",
    "        neg_samples = random.sample([\n",
    "            (i, j, self.R[i, j])\n",
    "            for i in range(self.num_users)\n",
    "            for j in range(self.num_items)\n",
    "            if self.R[i, j] == 0\n",
    "        ], len(pos_samples)*num_ng)\n",
    "        \n",
    "        self.samples = pos_samples + neg_samples\n",
    "        \n",
    "    def train(self, K, alpha=0.001, beta=0.01, epochs=20):\n",
    "        '''\n",
    "        - alpha (float) : learning rate\n",
    "        - beta (float)  : regularization parameter\n",
    "        - K (int)       : number of latent dimensions\n",
    "        -epochs(int)    : number of iterations\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.epochs = epochs\n",
    "        # Initialize user and item latent feature matrice\n",
    "        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))\n",
    "        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_i = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[np.where(self.R != 0)])\n",
    "               \n",
    "        # Perform stochastic gradient descent for number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            if (i+1) % 10 == 0:\n",
    "                mse = self.mse()\n",
    "                print(\"Iteration: %d ; error = %.4f\" % (i+1, mse))\n",
    "        \n",
    "        return self.full_matrix()\n",
    "\n",
    "    def mse(self):\n",
    "        \"\"\"\n",
    "        A function to compute the total mean square error\n",
    "        \"\"\"\n",
    "        xs, ys = self.R.nonzero()\n",
    "        predicted = self.full_matrix()\n",
    "        error = 0\n",
    "        for x, y in zip(xs, ys):\n",
    "            error += pow(self.R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(error)\n",
    "\n",
    "    def sgd(self):\n",
    "        \"\"\"\n",
    "        Perform stochastic graident descent\n",
    "        \"\"\"\n",
    "        for i, j, r in self.samples:\n",
    "            # Computer prediction and error\n",
    "            prediction = self.get_rating(i, j)\n",
    "            e = (r - prediction)\n",
    "            \n",
    "            # Update biases\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])\n",
    "            \n",
    "            # Create copy of row of P since we need to update it but use older values for update on Q\n",
    "            P_i = self.P[i, :][:]\n",
    "            \n",
    "            # Update user and item latent feature matrices\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * P_i - self.beta * self.Q[j,:])\n",
    "\n",
    "    def get_rating(self, i, j):\n",
    "        \"\"\"\n",
    "        Get the predicted rating of user i and item j\n",
    "        \"\"\"\n",
    "        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "    \n",
    "    def full_matrix(self):\n",
    "        \"\"\"\n",
    "        Computer the full matrix using the resultant biases, P and Q\n",
    "        \"\"\"\n",
    "        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics: Interaction = 994169, User = 6040, Item = 3706, Sparsity = 0.0444\n",
      "Iteration: 10 ; error = 1770.5244\n",
      "Iteration: 20 ; error = 1660.7947\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4027a348cdfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msquaredError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0msquaredError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquaredError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquaredError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName)#loading dataset\n",
    "        testset = random.sample(dataset.dataList, len(dataset.dataList)//10)#10% for test\n",
    "        svdb = SVDBias(R=dataset.list_to_matrix(), num_ng=2)#negative sample ratio\n",
    "        for K in [8, 16, 32, 64]:\n",
    "            nR = svdb.train(K=K, alpha=0.001, beta=0.01, epochs=20)\n",
    "            squaredError = []\n",
    "            for u,i,r in testset:\n",
    "                error=r - nR[int(u)][int(i)]\n",
    "                squaredError.append(error * error)\n",
    "            rmse =sqrt(sum(squaredError) / len(squaredError))\n",
    "            print(\"RMSE@{}:{} at Dataset {}\".format(K, rmse, fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE@8:1.6624953850640736 at Datasetml-1m\n"
     ]
    }
   ],
   "source": [
    "squaredError = []\n",
    "for u,i,r in testset:\n",
    "    error=r - nR[int(u)][int(i)]\n",
    "    squaredError.append(error * error)\n",
    "rmse =math.sqrt(sum(squaredError) / len(squaredError))\n",
    "print(\"RMSE@{}:{} at Dataset {}\".format(K, rmse, fileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
