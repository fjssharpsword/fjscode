{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-434ad07f3fe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Read data and build BMF model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTraindata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mtrain_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetInstances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegNum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mx_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mx_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-434ad07f3fe0>\u001b[0m in \u001b[0;36mgetInstances\u001b[0;34m(R, data, maxi, negNum)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mrate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.22\n",
    "@function: Implementing BMF(Bayesian Neural Collaborative Filtering) which is designed by Jason.F\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "\n",
    "def getTraindata():\n",
    "    data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                             usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "    maxu, maxi = data['user'].max(), data['item'].max()\n",
    "    data = data.values.tolist()\n",
    "    print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(maxu, maxi, len(data)))\n",
    "    R = np.zeros([maxu+1, maxi+1], dtype=np.float32)\n",
    "    for i in data:\n",
    "        user = int(i[0])\n",
    "        item = int(i[1])\n",
    "        rating = float(i[2])\n",
    "        R[user][item] = rating\n",
    "    return R, data, maxu, maxi\n",
    "\n",
    "def getTrainDict(data):\n",
    "    dataDict = {}\n",
    "    for i in data:\n",
    "        dataDict[(i[0], i[1])] = i[2]\n",
    "    return dataDict\n",
    "    \n",
    "def getInstances(R, data, maxi, negNum):\n",
    "    dataDict = getTrainDict(data)\n",
    "    user = []\n",
    "    item = []\n",
    "    rate = []\n",
    "    for i in data:\n",
    "        user.append(R[int(i[0]),:].tolist())\n",
    "        item.append(R[:,int(i[1])].tolist())\n",
    "        rate.append(1.0)\n",
    "        for t in range(negNum):\n",
    "            j = np.random.randint(maxi)\n",
    "            while (i[0], j) in dataDict:\n",
    "                j = np.random.randint(maxi)\n",
    "            user.append(R[int(i[0]),:].tolist())\n",
    "            item.append(R[:,j].tolist())\n",
    "            rate.append(0.0)\n",
    "    return np.array(user), np.array(item), np.array(rate)\n",
    "\n",
    "def getTestdata():\n",
    "    testset = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/ml-1m.test.negative'\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            testset.append([u, eval(arr[0])[1], 1.0])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                testset.append([u, int(i), 0.0]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return testset\n",
    "\n",
    "\n",
    "def getTestInstances(R, testset):\n",
    "    for i in testset:\n",
    "        user.append(R[int(i[0]),:].tolist())\n",
    "        item.append(R[:,int(i[1])].tolist())\n",
    "        rate.append(float(i[2]))\n",
    "    return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "def build_BNCF(x_u, x_i, y_r, maxu, maxi, K=8):\n",
    "    logging.info('building the BMF model')\n",
    "\n",
    "    Layers = [1024, K]\n",
    "    with pm.Model() as bncf:\n",
    "        #user layer\n",
    "        user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[maxi+1, Layer[0]] )\n",
    "        user_O1 = pm.math.tanh(pm.math.dot(x_u, user_W1))\n",
    "        user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[Layer[0],Layer[1]] )\n",
    "        user_O2 = pm.math.tanh(pm.math.dot(user_O1, user_W2))\n",
    "        #item layer\n",
    "        item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[maxu+1, Layer[0]] )\n",
    "        item_O1 = pm.math.tanh(pm.math.dot(x_i, item_W1))\n",
    "        item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[Layer[0],Layer[1]] )\n",
    "        item_O2 = pm.math.tanh(pm.math.dot(item_O1, item_W2))\n",
    "        #output layer\n",
    "        act_out = pm.math.sigmoid(pm.math.dot(user_O2, item_O2.T))\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        r = pm.Bernoulli('r', act_out, observed=y_r, total_size=y_r.shape[0]) # IMPORTANT for minibatches\n",
    "                                \n",
    "    logging.info('done building BMF model')\n",
    "    \n",
    "    return bncf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "\n",
    "    # Read data and build BMF model.\n",
    "    R, data, maxu, maxi = getTraindata()\n",
    "    train_u, train_i, train_r = getInstances(R, data, maxi, negNum=4)\n",
    "    x_u = theano.shared(train_u)\n",
    "    x_i = theano.shared(train_i)\n",
    "    y_r = theano.shared(train_r)\n",
    "    bncf = build_BNCF(x_u, x_i, y_r, maxu, maxi, K=8)#dim is the number of latent factors\n",
    "\n",
    "    with bncf:# sample with BMF\n",
    "        tstart = time.time()\n",
    "        logging.info('Start BMF sampling')\n",
    "        inference = pm.ADVI()\n",
    "        approx = pm.fit(n=1000, method=inference)\n",
    "        trace = approx.sample(draws=500)\n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Complete BMF sampling in %d seconds' % int(elapsed))\n",
    "   \n",
    "\n",
    "    testset = getTestdata()\n",
    "    test_u, test_i, test_r = getTestInstances(R, testset)\n",
    "    x_u.set_value(test_u)\n",
    "    x_i.set_value(test_i)\n",
    "    y_r.set_value(test_r)\n",
    "    with bncf:#evaluation\n",
    "        ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "        pre_r = ppc['r'].mean(axis=0)\n",
    "        \n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        prev_u = testset[0][0]\n",
    "        pos_i = testset[0][1]\n",
    "        scorelist = []\n",
    "        iLen = 0\n",
    "        for u, i in testset:\n",
    "            if prev_u == u:\n",
    "                scorelist.append([i,pre_r[iLen]])\n",
    "            else:\n",
    "                map_item_score = {}\n",
    "                for item, rate in scorelist: #turn dict\n",
    "                    map_item_score[item] = rate\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hr = getHitRatio(ranklist, pos_i)\n",
    "                hits.append(hr)\n",
    "                ndcg = getNDCG(ranklist, pos_i)\n",
    "                ndcgs.append(ndcg)\n",
    "                #next user\n",
    "                scorelist = []\n",
    "                prev_u = u\n",
    "                pos_i = i\n",
    "                scorelist.append([i,pre_r[iLen]])\n",
    "            iLen = iLen + 1\n",
    "        hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(\"hr: {}, NDCG: {}, At K {}\".format(hitratio, ndcg, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '15568' (I am process '16185')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /root/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.6.6-64/lock_dir\n",
      "/usr/local/lib/python3.6/dist-packages/pymc3/tuning/starting.py:61: UserWarning: find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.\n",
      "  warnings.warn('find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.')\n",
      "logp = -17,695, ||grad|| = 0.16015: 100%|██████████| 16/16 [00:00<00:00, 219.90it/s]  \n",
      "Multiprocess sampling (2 chains in 8 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [Q]\n",
      ">Metropolis: [P]\n",
      "Sampling 2 chains: 100%|██████████| 3000/3000 [00:06<00:00, 444.96draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 2000/2000 [00:03<00:00, 527.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE：0.627957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型MCMC采样-似然函数是正态\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False, iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "data_rating = data[['rating']]\n",
    "le = LabelEncoder()\n",
    "data = data[['userId','movieId']].apply(le.fit_transform)\n",
    "data = pd.concat([data,data_rating],axis=1)\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.构建概率模型\n",
    "#概率模型参数设置\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "X_input = theano.shared(data[['userId','movieId']].values)#转numpy array\n",
    "Y_output = theano.shared(data['rating'].values)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = pm.Deterministic('R', tt.dot(P,Q))\n",
    "    rY = []\n",
    "    for row in X_input.get_value(): # 获取每行的值\n",
    "        rr = R[int(row[0])][int(row[1])]#userId是0列,movieId是1列\n",
    "        rY.append(rr)\n",
    "    Y = pm.Normal('Y',mu=rY, sd=mean, observed=Y_output.get_value())\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    start=pm.find_MAP()  # 参数初猜\n",
    "    #二值变量：指定 BinaryMetropolis  离散变量：指定 Metropolis  连续变量：指定 NUTS\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000,start=start,step=step,chains=2,cores=8)\n",
    "\n",
    "#后验分布采样观察\n",
    "#pm.traceplot(trace, varnames=['P'])\n",
    "#pm.summary(trace, varnames=['P'])\n",
    "#print (trace['P'].shape)\n",
    "#print (trace['Q'].shape)\n",
    "#4.后验预测  \n",
    "#X_input.set_value(test[['userId','movieId']].values)#转numpy array\n",
    "#Y_output.set_value(test['rating'].values)\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace)#vars=BMF_model.observed_RVs\n",
    "    pred = ppc['Y'].mean(axis=0)\n",
    "    \n",
    "print ('RMSE：%f'% mean_squared_error(Y_output.get_value(),pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5,929.2: 100%|██████████| 10000/10000 [00:39<00:00, 254.78it/s]  \n",
      "Finished [100%]: Average Loss = 5,921.3\n",
      "100%|██████████| 5000/5000 [00:10<00:00, 481.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE：0.150872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型ADVI变分推断-似然函数是正态\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False, iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "data_rating = data[['rating']]\n",
    "le = LabelEncoder()\n",
    "data = data[['userId','movieId']].apply(le.fit_transform)\n",
    "data = pd.concat([data,data_rating],axis=1)\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.构建概率模型\n",
    "#概率模型参数设置\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "X_input = theano.shared(data[['userId','movieId']].values)#转numpy array\n",
    "Y_output = theano.shared(data['rating'].values)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = tt.dot(P,Q)\n",
    "    rY = []\n",
    "    for row in X_input.get_value(): # 获取每行的值\n",
    "        rr = R[int(row[0])][int(row[1])]#userId=0,movieId=1\n",
    "        rY.append(rr)\n",
    "    Y = pm.Normal('Y',mu=rY, sd=mean, observed=Y_output.get_value())\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=10000, method=inference)\n",
    "    trace = approx.sample(draws=5000)\n",
    "    \n",
    "#4.后验预测  \n",
    "#X_input.set_value(test[['userId','movieId']].values)#转numpy array\n",
    "#Y_output.set_value(test['rating'].values)\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace)\n",
    "    pred = ppc['Y'].mean(axis=0)\n",
    "    \n",
    "print ('RMSE：%f'% mean_squared_error(Y_output.get_value(),pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
