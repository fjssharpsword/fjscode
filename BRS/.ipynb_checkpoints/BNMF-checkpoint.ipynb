{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.25\n",
    "@function: BNMF(Bayesian Neural Matrix Factorization) \n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import heapq\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            data['rating']=data['rating'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            data['num']=data['num'].apply(lambda x: 1.0 if float(x)>0.0 else 0.0)\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))\n",
    "                item.append(int(i))\n",
    "                rate.append(float(0.0)) \n",
    "        return user, item, rate\n",
    "    \n",
    "    def getHitRatio(self, ranklist, targetItem):\n",
    "        for item in ranklist:\n",
    "            if item == targetItem:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def getNDCG(self, ranklist, targetItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == targetItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "    \n",
    "class BNCF:\n",
    "    def __init__(self, dataset):\n",
    "        self.maxu = dataset.maxu\n",
    "        self.maxi = dataset.maxi\n",
    "        self.testList = dataset.testList\n",
    "        \n",
    "        #get the trainset and testset\n",
    "        dataMat = dataset.list_to_matrix()\n",
    "        #get the test data\n",
    "        test_u, test_i, test_r = dataset.getInstances(isTest=True)\n",
    "        self.test_u = dataMat[test_u,:]\n",
    "        self.test_i = dataMat.T[test_i,:]\n",
    "        self.test_r = np.array(test_r)\n",
    "        assert(len(self.test_u) == len(self.test_i) and len(self.test_i) == len(self.test_r))\n",
    "        #get the training data and setting the input \n",
    "        train_u, train_i, train_r = dataset.getInstances(isTest=False)\n",
    "        self.train_u = dataMat[train_u,:]\n",
    "        self.train_i = dataMat.T[train_i,:]\n",
    "        self.train_r = np.array(train_r)\n",
    "        assert(len(self.train_u) == len(self.train_i) and len(self.train_i) == len(self.train_r)) \n",
    "        #release memory\n",
    "        del dataset, dataMat, train_u, train_i, train_r, test_u, test_i, test_r\n",
    "        gc.collect()\n",
    "        \n",
    "    def build_BNCF(self, K = 8):\n",
    "        layers = [1024, K] #number of latent factors\n",
    "        print('start building the BNCF model')\n",
    "        \n",
    "        self.x_u = theano.shared(self.train_u)#self.x_u = pm.Minibatch(self.train_u, batch_size=8096)\n",
    "        self.x_i = theano.shared(self.train_i)#self.x_i = pm.Minibatch(self.train_i, batch_size=8096)\n",
    "        self.y_r = theano.shared(self.train_r)#self.y_r = pm.Minibatch(self.train_r, batch_size=8096)\n",
    "        #release memory\n",
    "        del self.train_u, self.train_i, self.train_r\n",
    "        gc.collect()\n",
    "        with pm.Model() as self.bncf:\n",
    "            #user layer\n",
    "            user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[self.maxi, layers[0]] )\n",
    "            user_O1 = pm.math.tanh(pm.math.dot(self.x_u, user_W1))\n",
    "            user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            user_O2 = pm.math.tanh(pm.math.dot(user_O1, user_W2))\n",
    "            #item layer\n",
    "            item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[self.maxu, layers[0]] )\n",
    "            item_O1 = pm.math.tanh(pm.math.dot(self.x_i, item_W1))\n",
    "            item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            item_O2 = pm.math.tanh(pm.math.dot(item_O1, item_W2))\n",
    "            #output layer\n",
    "            act_out = pm.math.sigmoid(np.multiply(user_O2, item_O2).sum(axis=1, keepdims=True))\n",
    "            # Binary classification -> Bernoulli likelihood\n",
    "            r = pm.Bernoulli('r', act_out, observed=self.y_r)                       \n",
    "        print('done building BNCF model')\n",
    "                \n",
    "    def train_BNCF(self):\n",
    "        print('start training the BNCF model')\n",
    "        tstart = time.time()\n",
    "        with self.bncf:\n",
    "            inference = pm.ADVI()\n",
    "            approx = pm.fit(n=1000, method=inference)\n",
    "            self.trace = approx.sample(draws=500)       \n",
    "        elapsed = time.time() - tstart    \n",
    "        print('Completed training the BNCF model in %d seconds' % int(elapsed))\n",
    "           \n",
    "    def evaluate_BNCF(self):\n",
    "        self.x_u.set_value(self.test_u)\n",
    "        self.x_i.set_value(self.test_i)\n",
    "        self.y_r.set_value(self.test_r)\n",
    "        #release memory\n",
    "        del self.test_u, self.test_i, self.test_r\n",
    "        gc.collect()\n",
    "        with self.bncf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True)\n",
    "            pre_r = ppc['r'].mean(axis=0)\n",
    "\n",
    "            hits = []\n",
    "            ndcgs = []\n",
    "            prev_u = self.testList[0][0]\n",
    "            pos_i = self.testList[0][1]\n",
    "            scorelist = []\n",
    "            iLen = 0\n",
    "            for u, i in self.testList:\n",
    "                if prev_u == u:\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                else:\n",
    "                    map_item_score = {}\n",
    "                    for item, rate in scorelist: #turn dict\n",
    "                        map_item_score[item] = rate\n",
    "                    ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                    hits.append(self.dataset.getHitRatio(ranklist, pos_i))\n",
    "                    ndcgs.append(self.dataset.getNDCG(ranklist, pos_i))\n",
    "                    #next user\n",
    "                    scorelist = []\n",
    "                    prev_u = u\n",
    "                    pos_i = i\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                iLen = iLen + 1\n",
    "            hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "            return hit, ndcg\n",
    "if __name__ == \"__main__\":\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        model = BNCF(dataset)\n",
    "        for K in [8, 16, 32, 64]:\n",
    "            model.build_BNCF(K)\n",
    "            model.train_BNCF()\n",
    "            hit, ndcg = model.evaluate_BNCF()\n",
    "            print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset {}\".format(hit, ndcg, K, fileName ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.23\n",
    "@function: Implementing BMF(Bayesian Neural Collaborative Filtering) which is designed by Jason.F\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import tensorflow as tf\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, fileName, negNum):\n",
    "        self.negNum = negNum #negative sample ratio\n",
    "        self.trainList, self.maxu, self.maxi = self.getTrainset_as_list(fileName)\n",
    "        self.testList = self.getTestset_as_list(fileName)\n",
    "        \n",
    "    def getTrainset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".train.rating\" \n",
    "            data = pd.read_csv(filePath, sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                                 usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_trainset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            maxu, maxi = data['user'].max()+1, data['item'].max()+1\n",
    "            print('Dataset Statistics: Interaction = %d, User = %d, Item = %d, Sparsity = %.4f' % \\\n",
    "                  (data.shape[0], maxu, maxi, data.shape[0]/(maxu*maxi)))\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList, maxu, maxi\n",
    "    \n",
    "    def getTestset_as_list(self, fileName):\n",
    "        if (fileName == 'ml-1m') or (fileName == 'pinterest-20'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/ml/\"+fileName+\".test.negative\" \n",
    "            dataList = []\n",
    "            with open(filePath, 'r') as fd:\n",
    "                line = fd.readline()\n",
    "                while line != None and line != '':\n",
    "                    arr = line.split('\\t')\n",
    "                    u = eval(arr[0])[0]\n",
    "                    dataList.append([u, eval(arr[0])[1], 1.0])#first is one postive item\n",
    "                    for i in arr[1:]:\n",
    "                        dataList.append([u, int(i), 0.0]) #99 negative items\n",
    "                    line = fd.readline()\n",
    "            return dataList\n",
    "        if (fileName == 'kb-cc'):\n",
    "            filePath = \"/data/fjsdata/ctKngBase/kbcc_testset.csv\"\n",
    "            data = pd.read_csv(filePath, sep='|', low_memory=False, dtype={'csr':int, 'ke':int, 'num':float})\n",
    "            dataList = data.values.tolist()\n",
    "            return dataList\n",
    "        \n",
    "    def list_to_matrix(self):              \n",
    "        dataMat = np.zeros([self.maxu, self.maxi], dtype=np.float32)\n",
    "        for u,i,r in self.trainList:\n",
    "            dataMat[int(u)][int(i)] = float(1.0)#float(r)\n",
    "        return np.array(dataMat)\n",
    "    \n",
    "    def list_to_dict(self):\n",
    "        dataDict = {}\n",
    "        for u,i,r in self.trainList:\n",
    "            dataDict[int(u), int(i)] = float(1.0)#float(r)\n",
    "        return dataDict\n",
    "    \n",
    "    def getInstances(self, isTest=False):\n",
    "        #dataMat = self.list_to_matrix(self.trainList, self.maxu, self.maxi)\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        if isTest==True: #test\n",
    "            for u, i, r in self.testList:\n",
    "                user.append(int(u))#user.append(dataMat[int(u),:].tolist())\n",
    "                item.append(int(i))#item.append(dataMat[:,int(i)].tolist())\n",
    "                rate.append(1.0)#rate.append(float(r))\n",
    "        else:#train\n",
    "            for u, i, r in self.trainList:\n",
    "                user.append(int(u))#user.append(dataMat[int(u),:].tolist())\n",
    "                item.append(int(i))#item.append(dataMat[:,int(i)].tolist())\n",
    "                rate.append(1.0)#rate.append(float(r))\n",
    "            #negative samples\n",
    "            dataDict = self.list_to_dict()\n",
    "            for j in range(len(self.trainList)*self.negNum):\n",
    "                u = np.random.randint(self.maxu)\n",
    "                i = np.random.randint(self.maxi)\n",
    "                while (u, i) in dataDict:\n",
    "                    u = np.random.randint(self.maxu)\n",
    "                    i = np.random.randint(self.maxi)\n",
    "                user.append(int(u))#user.append(dataMat[u,:].tolist())\n",
    "                item.append(int(i))#item.append(dataMat[:,i].tolist())\n",
    "                rate.append(0.0) \n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "    def getHitRatio(self, ranklist, targetItem):\n",
    "        for item in ranklist:\n",
    "            if item == targetItem:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    def getNDCG(self, ranklist, targetItem):\n",
    "        for i in range(len(ranklist)):\n",
    "            item = ranklist[i]\n",
    "            if item == targetItem:\n",
    "                return math.log(2) / math.log(i+2)\n",
    "        return 0\n",
    "class BNCF:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        #self.trainList = self.dataset.trainList\n",
    "        #self.testList = self.dataset.testList\n",
    "        self.maxu = self.dataset.maxu\n",
    "        self.maxi = self.dataset.maxi\n",
    "        \n",
    "        #get the test data\n",
    "        self.test_u, self.test_i, self.test_r = self.dataset.getInstances(isTest=True)\n",
    "        #get the training data and setting the input \n",
    "        self.train_u, self.train_i, self.train_r = self.dataset.getInstances(isTest=False)\n",
    "        assert(self.train_u.shape == self.train_i.shape and self.train_i.shape == self.train_r.shape)\n",
    "        #initiate the seesion\n",
    "        self.init_sess()\n",
    "        #train data by mini-batch\n",
    "        self.arrUser, self.arrItem = self.batchTrainset()\n",
    "       \n",
    "    def init_sess(self):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.R = tf.placeholder(tf.float32, shape=(self.maxu, self.maxi))\n",
    "        #embedding layer\n",
    "        self.user_item_embedding = tf.convert_to_tensor(self.R)\n",
    "        self.item_user_embedding = tf.transpose(self.user_item_embedding)\n",
    "        self.user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)\n",
    "        self.item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)\n",
    "        #define seesion\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def batchTrainset(self):\n",
    "        #get input in the way of mini-batch\n",
    "        batchSize=8192\n",
    "        num_batches = len(self.train_u) // batchSize + 1\n",
    "        train_u_batch = self.train_u[0: batchSize]\n",
    "        train_i_batch = self.train_i[0: batchSize]\n",
    "        arrUser, arrItem = self.sess.run([self.user_input, self.item_input], \\\n",
    "                                             feed_dict={self.user: train_u_batch, \\\n",
    "                                                        self.item: train_i_batch, \\\n",
    "                                                        self.R: self.dataset.list_to_matrix()})\n",
    "        '''\n",
    "        for i in range(1, num_batches):\n",
    "            min_idx = i * batchSize\n",
    "            max_idx = np.min([len(self.train_u), (i+1)*batchSize])\n",
    "            train_u_batch = self.train_u[min_idx: max_idx]\n",
    "            train_i_batch = self.train_i[min_idx: max_idx]\n",
    "            train_r_batch = self.train_r[min_idx: max_idx]\n",
    "            batchUser, batchItem = self.sess.run([self.user_input, self.item_input], \\\n",
    "                                             feed_dict={self.user: train_u_batch, \\\n",
    "                                                        self.item: train_i_batch, \\\n",
    "                                                        self.R: self.dataset.list_to_matrix()})\n",
    "            arrUser = np.concatenate((arrUser, batchUser),axis=0)\n",
    "            arrItem = np.concatenate((arrItem, batchItem),axis=0)\n",
    "        '''\n",
    "        return arrUser, arrItem\n",
    "    \n",
    "    \n",
    "    def build_BNCF(self, K = 8):\n",
    "        layers = [1024, K] #number of latent factors\n",
    "        logging.info('start building the BNCF model')\n",
    "        \n",
    "        self.x_u = theano.shared(self.arrUser)\n",
    "        self.x_i = theano.shared(self.arrItem)\n",
    "        self.y_r = theano.shared(self.train_r)       \n",
    "        with pm.Model() as self.bncf:\n",
    "            #user layer\n",
    "            user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[self.maxi, layers[0]] )\n",
    "            user_O1 = pm.math.tanh(pm.math.dot(self.x_u, user_W1))\n",
    "            user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            user_O2 = pm.math.tanh(pm.math.dot(user_O1, user_W2))\n",
    "            #item layer\n",
    "            item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[self.maxu, layers[0]] )\n",
    "            item_O1 = pm.math.tanh(pm.math.dot(self.x_i, item_W1))\n",
    "            item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[layers[0],layers[1]] )\n",
    "            item_O2 = pm.math.tanh(pm.math.dot(item_O1, item_W2))\n",
    "            #output layer\n",
    "            #act_out = pm.math.sigmoid(pm.math.dot(user_O2, item_O2.T))\n",
    "            #act_out = pm.math.sigmoid(np.sum(np.multiply(user_O2, item_O2),axis=1, keepdims=True))\n",
    "            act_out = pm.math.sigmoid(np.multiply(user_O2, item_O2).sum(axis=1, keepdims=True))\n",
    "            #act_out = pm.math.sigmoid(tf.reduce_sum(tf.multiply(user_O2, item_O2),axis=1, keep_dims=True))\n",
    "            # Binary classification -> Bernoulli likelihood\n",
    "            r = pm.Bernoulli('r', act_out, observed=self.y_r, total_size=self.y_r.shape[0]) # IMPORTANT for minibatches                         \n",
    "        logging.info('done building BNCF model')\n",
    "                \n",
    "    def train_BNCF(self):\n",
    "        logging.info('start training the BNCF model')\n",
    "        tstart = time.time()\n",
    "        with self.bncf:\n",
    "            inference = pm.ADVI()\n",
    "            approx = pm.fit(n=1000, method=inference)\n",
    "            self.trace = approx.sample(draws=500)       \n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Completed training the BNCF model in %d seconds' % int(elapsed))\n",
    "           \n",
    "    def evaluate_BNCF(self):\n",
    "        arrUser, arrItem = self.sess.run([self.user_input, self.item_input], \\\n",
    "                               feed_dict={self.user: self.test_u, \\\n",
    "                                          self.item: self.test_i, \\\n",
    "                                          self.R: self.dataset.list_to_matrix()})\n",
    "        self.x_u.set_value(arrUser)\n",
    "        self.x_i.set_value(arrItem)\n",
    "        self.y_r.set_value(self.test_r)\n",
    "        with self.bncf:#evaluation\n",
    "            ppc = pm.sample_posterior_predictive(self.trace, progressbar=True)\n",
    "            pre_r = ppc['r'].mean(axis=0)\n",
    "\n",
    "            hits = []\n",
    "            ndcgs = []\n",
    "            prev_u = self.dataset.testList[0][0]\n",
    "            pos_i = self.dataset.testList[0][1]\n",
    "            scorelist = []\n",
    "            iLen = 0\n",
    "            for u, i in self.dataset.testList:\n",
    "                if prev_u == u:\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                else:\n",
    "                    map_item_score = {}\n",
    "                    for item, rate in scorelist: #turn dict\n",
    "                        map_item_score[item] = rate\n",
    "                    ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                    hits.append(self.dataset.getHitRatio(ranklist, pos_i))\n",
    "                    ndcgs.append(self.dataset.getNDCG(ranklist, pos_i))\n",
    "                    #next user\n",
    "                    scorelist = []\n",
    "                    prev_u = u\n",
    "                    pos_i = i\n",
    "                    scorelist.append([i,pre_r[iLen]])\n",
    "                iLen = iLen + 1\n",
    "            hit, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "            return hit, ndcg\n",
    "if __name__ == \"__main__\":\n",
    "    for fileName in ['ml-1m', 'pinterest-20', 'kb-cc']:\n",
    "        dataset = DataSet(fileName=fileName, negNum=4)#loading dataset\n",
    "        model = BNCF(dataset)\n",
    "        for K in [8, 16, 32, 64]:\n",
    "            model.build_BNCF(K)\n",
    "            model.train_BNCF()\n",
    "            hit, ndcg = model.evaluate_BNCF()\n",
    "            print(\"HR@10: {}, NDCG@10: {}, At K {} and Dataset {}\".format(hit, ndcg, K, fileName ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6039\n",
      "\tItem Num: 3705\n",
      "\tData Size: 994169\n"
     ]
    }
   ],
   "source": [
    "# -*- Encoding:UTF-8 -*-\n",
    "'''\n",
    "@author: Jason.F\n",
    "@data: 2019.07.22\n",
    "@function: Implementing BMF(Bayesian Neural Collaborative Filtering) which is designed by Jason.F\n",
    "           Dataset: Movielen Dataset(ml-1m) \n",
    "           Evaluating: hitradio,ndcg\n",
    "'''\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import tensorflow as tf\n",
    "\n",
    "def getTraindata():\n",
    "    data = pd.read_csv(\"/data/fjsdata/ctKngBase/ml/ml-1m.train.rating\", \\\n",
    "                             sep='\\t', header=None, names=['user', 'item', 'rating'], \\\n",
    "                             usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.float})\n",
    "    maxu, maxi = data['user'].max(), data['item'].max()\n",
    "    data = data.values.tolist()\n",
    "    print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(maxu, maxi, len(data)))\n",
    "    R = np.zeros([maxu+1, maxi+1], dtype=np.float32)\n",
    "    for i in data:\n",
    "        user = int(i[0])\n",
    "        item = int(i[1])\n",
    "        rating = float(i[2])\n",
    "        R[user][item] = rating\n",
    "    return R, data, maxu, maxi\n",
    "\n",
    "def getTrainDict(data):\n",
    "    dataDict = {}\n",
    "    for i in data:\n",
    "        dataDict[(i[0], i[1])] = i[2]\n",
    "    return dataDict\n",
    "    \n",
    "def getInstances(R, data, maxu, maxi, negNum):\n",
    "    user = []\n",
    "    item = []\n",
    "    rate = []\n",
    "    '''\n",
    "    for i in data:\n",
    "        user.append(R[int(i[0]),:].tolist())\n",
    "        item.append(R[:,int(i[1])].tolist())\n",
    "        rate.append(1.0)\n",
    "        for t in range(negNum):\n",
    "            j = np.random.randint(maxi)\n",
    "            while (i[0], j) in dataDict:\n",
    "                j = np.random.randint(maxi)\n",
    "            user.append(R[int(i[0]),:].tolist())\n",
    "            item.append(R[:,j].tolist())\n",
    "            rate.append(0.0)\n",
    "    '''\n",
    "    for u, i, _ in data:\n",
    "        user.append(R[int(u),:].tolist())\n",
    "        item.append(R[:,int(i)].tolist())\n",
    "        rate.append(1.0)\n",
    "    '''\n",
    "    dataDict = getTrainDict(data)\n",
    "    for j in range(len(data)*negNum):\n",
    "        u = np.random.randint(maxu)\n",
    "        i = np.random.randint(maxi)\n",
    "        while (u, i) in dataDict:\n",
    "            u = np.random.randint(maxu)\n",
    "            i = np.random.randint(maxi)\n",
    "        user.append(R[u,:].tolist())\n",
    "        item.append(R[:,i].tolist())\n",
    "        rate.append(0.0)\n",
    "    '''\n",
    "    return np.array(user), np.array(item), np.array(rate)\n",
    "\n",
    "def getTestdata():\n",
    "    testset = []\n",
    "    filePath = '/data/fjsdata/ctKngBase/ml/ml-1m.test.negative'\n",
    "    with open(filePath, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        while line != None and line != '':\n",
    "            arr = line.split('\\t')\n",
    "            u = eval(arr[0])[0]\n",
    "            testset.append([u, eval(arr[0])[1], 1.0])#one postive item\n",
    "            for i in arr[1:]:\n",
    "                testset.append([u, int(i), 0.0]) #99 negative items\n",
    "            line = fd.readline()\n",
    "    return testset\n",
    "\n",
    "\n",
    "def getTestInstances(R, testset):\n",
    "    for i in testset:\n",
    "        user.append(R[int(i[0]),:].tolist())\n",
    "        item.append(R[:,int(i[1])].tolist())\n",
    "        rate.append(float(i[2]))\n",
    "    return np.array(user), np.array(item), np.array(rate)\n",
    "    \n",
    "def build_BNCF(x_u, x_i, y_r, maxu, maxi, K=8):\n",
    "    logging.info('building the BMF model')\n",
    "\n",
    "    Layers = [1024, K]\n",
    "    with pm.Model() as bncf:\n",
    "        #user layer\n",
    "        user_W1 = pm.Normal('user_W1', 0, sd=1, shape=[maxi+1, Layer[0]] )\n",
    "        user_O1 = pm.math.tanh(pm.math.dot(x_u, user_W1))\n",
    "        user_W2 = pm.Normal('user_W2', 0, sd=1, shape=[Layer[0],Layer[1]] )\n",
    "        user_O2 = pm.math.tanh(pm.math.dot(user_O1, user_W2))\n",
    "        #item layer\n",
    "        item_W1 = pm.Normal('item_W1', 0, sd=1, shape=[maxu+1, Layer[0]] )\n",
    "        item_O1 = pm.math.tanh(pm.math.dot(x_i, item_W1))\n",
    "        item_W2 = pm.Normal('item_W2', 0, sd=1, shape=[Layer[0],Layer[1]] )\n",
    "        item_O2 = pm.math.tanh(pm.math.dot(item_O1, item_W2))\n",
    "        #output layer\n",
    "        act_out = pm.math.sigmoid(pm.math.dot(user_O2, item_O2.T))\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        r = pm.Bernoulli('r', act_out, observed=y_r, total_size=y_r.shape[0]) # IMPORTANT for minibatches\n",
    "                                \n",
    "    logging.info('done building BMF model')\n",
    "    \n",
    "    return bncf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO,format='[%(asctime)s]: %(message)s')\n",
    "\n",
    "    # Read data and build BMF model.\n",
    "    R, data, maxu, maxi = getTraindata()\n",
    "    train_u, train_i, train_r = getInstances(R, data, maxu, maxi, negNum=4)\n",
    "    x_u = theano.shared(train_u)\n",
    "    x_i = theano.shared(train_i)\n",
    "    y_r = theano.shared(train_r)\n",
    "    bncf = build_BNCF(x_u, x_i, y_r, maxu, maxi, K=8)#dim is the number of latent factors\n",
    "\n",
    "    with bncf:# sample with BMF\n",
    "        tstart = time.time()\n",
    "        logging.info('Start BMF sampling')\n",
    "        inference = pm.ADVI()\n",
    "        approx = pm.fit(n=1000, method=inference)\n",
    "        trace = approx.sample(draws=500)\n",
    "        elapsed = time.time() - tstart    \n",
    "        logging.info('Complete BMF sampling in %d seconds' % int(elapsed))\n",
    "   \n",
    "\n",
    "    testset = getTestdata()\n",
    "    test_u, test_i, test_r = getTestInstances(R, testset)\n",
    "    x_u.set_value(test_u)\n",
    "    x_i.set_value(test_i)\n",
    "    y_r.set_value(test_r)\n",
    "    with bncf:#evaluation\n",
    "        ppc = pm.sample_posterior_predictive(trace, progressbar=True)\n",
    "        pre_r = ppc['r'].mean(axis=0)\n",
    "        \n",
    "        hits = []\n",
    "        ndcgs = []\n",
    "        prev_u = testset[0][0]\n",
    "        pos_i = testset[0][1]\n",
    "        scorelist = []\n",
    "        iLen = 0\n",
    "        for u, i in testset:\n",
    "            if prev_u == u:\n",
    "                scorelist.append([i,pre_r[iLen]])\n",
    "            else:\n",
    "                map_item_score = {}\n",
    "                for item, rate in scorelist: #turn dict\n",
    "                    map_item_score[item] = rate\n",
    "                ranklist = heapq.nlargest(10, map_item_score, key=map_item_score.get)#default Topn=10\n",
    "                hr = getHitRatio(ranklist, pos_i)\n",
    "                hits.append(hr)\n",
    "                ndcg = getNDCG(ranklist, pos_i)\n",
    "                ndcgs.append(ndcg)\n",
    "                #next user\n",
    "                scorelist = []\n",
    "                prev_u = u\n",
    "                pos_i = i\n",
    "                scorelist.append([i,pre_r[iLen]])\n",
    "            iLen = iLen + 1\n",
    "        hitratio,ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "        print(\"hr: {}, NDCG: {}, At K {}\".format(hitratio, ndcg, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '15568' (I am process '16185')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /root/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.6.6-64/lock_dir\n",
      "/usr/local/lib/python3.6/dist-packages/pymc3/tuning/starting.py:61: UserWarning: find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.\n",
      "  warnings.warn('find_MAP should not be used to initialize the NUTS sampler, simply call pymc3.sample() and it will automatically initialize NUTS in a better way.')\n",
      "logp = -17,695, ||grad|| = 0.16015: 100%|██████████| 16/16 [00:00<00:00, 219.90it/s]  \n",
      "Multiprocess sampling (2 chains in 8 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [Q]\n",
      ">Metropolis: [P]\n",
      "Sampling 2 chains: 100%|██████████| 3000/3000 [00:06<00:00, 444.96draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "100%|██████████| 2000/2000 [00:03<00:00, 527.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE：0.627957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型MCMC采样-似然函数是正态\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False, iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "data_rating = data[['rating']]\n",
    "le = LabelEncoder()\n",
    "data = data[['userId','movieId']].apply(le.fit_transform)\n",
    "data = pd.concat([data,data_rating],axis=1)\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.构建概率模型\n",
    "#概率模型参数设置\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "X_input = theano.shared(data[['userId','movieId']].values)#转numpy array\n",
    "Y_output = theano.shared(data['rating'].values)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = pm.Deterministic('R', tt.dot(P,Q))\n",
    "    rY = []\n",
    "    for row in X_input.get_value(): # 获取每行的值\n",
    "        rr = R[int(row[0])][int(row[1])]#userId是0列,movieId是1列\n",
    "        rY.append(rr)\n",
    "    Y = pm.Normal('Y',mu=rY, sd=mean, observed=Y_output.get_value())\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    start=pm.find_MAP()  # 参数初猜\n",
    "    #二值变量：指定 BinaryMetropolis  离散变量：指定 Metropolis  连续变量：指定 NUTS\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000,start=start,step=step,chains=2,cores=8)\n",
    "\n",
    "#后验分布采样观察\n",
    "#pm.traceplot(trace, varnames=['P'])\n",
    "#pm.summary(trace, varnames=['P'])\n",
    "#print (trace['P'].shape)\n",
    "#print (trace['Q'].shape)\n",
    "#4.后验预测  \n",
    "#X_input.set_value(test[['userId','movieId']].values)#转numpy array\n",
    "#Y_output.set_value(test['rating'].values)\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace)#vars=BMF_model.observed_RVs\n",
    "    pred = ppc['Y'].mean(axis=0)\n",
    "    \n",
    "print ('RMSE：%f'% mean_squared_error(Y_output.get_value(),pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 5,929.2: 100%|██████████| 10000/10000 [00:39<00:00, 254.78it/s]  \n",
      "Finished [100%]: Average Loss = 5,921.3\n",
      "100%|██████████| 5000/5000 [00:10<00:00, 481.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE：0.150872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#矩阵分解R=PQ，推荐概率模型ADVI变分推断-似然函数是正态\n",
    "import theano\n",
    "import pymc3 as pm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#1.数据集处理\n",
    "#http://files.grouplens.org/datasets/movielens/ml-20m-README.html\n",
    "#the following format of file ratings.csv: userId,movieId,rating,timestamp\n",
    "#The lines within this file are ordered first by userId, then, within user, by movieId.\n",
    "#Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "#Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "data = pd.read_csv(\"/data/fjsdata/BayesianRS/ml-20m/ratings.csv\",sep=',',low_memory=False, iterator =True)\n",
    "data = data.get_chunk(100)\n",
    "#将userId和movieId全部标准编号\n",
    "data_rating = data[['rating']]\n",
    "le = LabelEncoder()\n",
    "data = data[['userId','movieId']].apply(le.fit_transform)\n",
    "data = pd.concat([data,data_rating],axis=1)\n",
    "#抽样10%比例测试\n",
    "test = data.sample(frac=0.1)\n",
    "#2.构建概率模型\n",
    "#概率模型参数设置\n",
    "uNum = len(data['userId'].unique())#统计用户数\n",
    "iNum = len(data['movieId'].unique())#统计电影数\n",
    "mean= data['rating'].max()/2 #正态分布的均值和方差\n",
    "k = 100 #隐因子数\n",
    "X_input = theano.shared(data[['userId','movieId']].values)#转numpy array\n",
    "Y_output = theano.shared(data['rating'].values)#转numpy array\n",
    "with pm.Model() as BMF_model:\n",
    "    # Creating the model\n",
    "    P = pm.Normal('P', mu=mean, sd=mean, shape=(uNum,k))\n",
    "    Q = pm.Normal('Q', mu=mean, sd=mean, shape=(k,iNum))\n",
    "    R = tt.dot(P,Q)\n",
    "    rY = []\n",
    "    for row in X_input.get_value(): # 获取每行的值\n",
    "        rr = R[int(row[0])][int(row[1])]#userId=0,movieId=1\n",
    "        rY.append(rr)\n",
    "    Y = pm.Normal('Y',mu=rY, sd=mean, observed=Y_output.get_value())\n",
    "#3.后验分布计算  \n",
    "with BMF_model:        \n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=10000, method=inference)\n",
    "    trace = approx.sample(draws=5000)\n",
    "    \n",
    "#4.后验预测  \n",
    "#X_input.set_value(test[['userId','movieId']].values)#转numpy array\n",
    "#Y_output.set_value(test['rating'].values)\n",
    "with BMF_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace)\n",
    "    pred = ppc['Y'].mean(axis=0)\n",
    "    \n",
    "print ('RMSE：%f'% mean_squared_error(Y_output.get_value(),pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
